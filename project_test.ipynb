{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas_datareader as dr\n",
    "\n",
    "df=dr.DataReader('SPY',data_source='yahoo',start='2007-1-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df.iloc[:2500, 1:2].values\n",
    "test_set = df.iloc[2500:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "# Creating a data structure with 60 time-steps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 800):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "#(740, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47729193, 0.48326944, 0.49256063, 0.49418496, 0.49710873,\n",
       "       0.50061723, 0.50139697, 0.49665387, 0.49535443, 0.5019817 ,\n",
       "       0.51146781, 0.51686049, 0.51634079, 0.51497631, 0.51939443,\n",
       "       0.52543698, 0.52121377, 0.52576186, 0.53277896, 0.5310896 ,\n",
       "       0.52699636, 0.52348776, 0.53050486, 0.53687219, 0.54005591,\n",
       "       0.54388929, 0.53947118, 0.54103046, 0.53388349, 0.53680726,\n",
       "       0.53726202, 0.539861  , 0.5410955 , 0.5458385 , 0.5503866 ,\n",
       "       0.55486975, 0.55409011, 0.55396013, 0.54343453, 0.54629326,\n",
       "       0.54804756, 0.54733286, 0.55889805, 0.56143199, 0.56123708,\n",
       "       0.55720879, 0.54999678, 0.53251901, 0.53271392, 0.54317459,\n",
       "       0.53570273, 0.53680726, 0.55181601, 0.55766355, 0.55590935,\n",
       "       0.55396013, 0.54486395, 0.54025082, 0.53765194, 0.53225916,\n",
       "       0.52745112, 0.52602171, 0.53648237, 0.53310375, 0.54362945,\n",
       "       0.55155616, 0.54921713, 0.55116624, 0.5556494 , 0.54362945,\n",
       "       0.54200512, 0.55383016, 0.56714965, 0.56838416, 0.56903383,\n",
       "       0.56006762, 0.56948869, 0.55701388, 0.56006762, 0.54356441,\n",
       "       0.54025082, 0.51517122, 0.50646486, 0.50802415, 0.50639982,\n",
       "       0.49931779, 0.50782924, 0.49444481, 0.49009163, 0.50763433,\n",
       "       0.52134364, 0.50802415, 0.49392501, 0.50626995, 0.49132613,\n",
       "       0.47768175, 0.45416154, 0.4826847 , 0.49502954, 0.50055229,\n",
       "       0.5082841 , 0.51010333, 0.51445652, 0.51738028, 0.49613417,\n",
       "       0.49938282, 0.50815412, 0.51803005, 0.52173346, 0.51913458,\n",
       "       0.51991423, 0.50782924, 0.50178679, 0.51348196, 0.51900461,\n",
       "       0.52693133, 0.52394262, 0.52322792, 0.52647657, 0.55623423,\n",
       "       0.55233581, 0.54993184, 0.54746283, 0.54168023, 0.54765774,\n",
       "       0.5528556 , 0.55149113, 0.55486975, 0.56338121, 0.55818334,\n",
       "       0.56195179, 0.56870904, 0.56961866, 0.57130792, 0.57377693,\n",
       "       0.56812421, 0.57286731, 0.56422589, 0.56117215, 0.55467484,\n",
       "       0.5586382 , 0.53641743, 0.52992013, 0.54025082, 0.5310896 ,\n",
       "       0.53784685, 0.55097133, 0.56169194, 0.55727372, 0.55707881,\n",
       "       0.54245988, 0.53349367, 0.53193428, 0.53797672, 0.52270812,\n",
       "       0.50659483, 0.50542527, 0.49769346, 0.50756939, 0.51770517,\n",
       "       0.5030213 , 0.50334618, 0.49437987, 0.48736277, 0.48450394,\n",
       "       0.49119616, 0.4779417 , 0.47982587, 0.50055229, 0.51328705,\n",
       "       0.5212787 , 0.52101876, 0.51465143, 0.52452736, 0.5293354 ,\n",
       "       0.54220003, 0.54421418, 0.52452736, 0.52043402, 0.52108379,\n",
       "       0.51978435, 0.50523036, 0.49938282, 0.50575015, 0.50730944,\n",
       "       0.51971932, 0.52875056, 0.52888054, 0.52121377, 0.51848481,\n",
       "       0.5130271 , 0.49886303, 0.50009753, 0.47956603, 0.47430323,\n",
       "       0.46351768, 0.45870964, 0.46956012, 0.46715615, 0.47625234,\n",
       "       0.46000908, 0.44948348, 0.42771747, 0.41582749, 0.3826912 ,\n",
       "       0.38814891, 0.43018648, 0.42557345, 0.42206485, 0.44038729,\n",
       "       0.43856806, 0.42947177, 0.45754017, 0.45831982, 0.43252551,\n",
       "       0.42433894, 0.41992073, 0.4223248 , 0.42193487, 0.43453966,\n",
       "       0.44207656, 0.43980246, 0.4340849 , 0.43863299, 0.43311024,\n",
       "       0.43512449, 0.42726271, 0.43973752, 0.45091289, 0.45682546,\n",
       "       0.45123778, 0.42674291, 0.42323441, 0.41511278, 0.42388409,\n",
       "       0.41225395, 0.39945426, 0.39302189, 0.40185823, 0.41621731,\n",
       "       0.39958423, 0.3942564 , 0.38314601, 0.40855044, 0.40894027,\n",
       "       0.40387238, 0.42973172, 0.43317528, 0.42888704, 0.42401406,\n",
       "       0.41556754, 0.41576246, 0.43148592, 0.44753427, 0.44578007,\n",
       "       0.44844389, 0.45247228, 0.44733936, 0.44045223, 0.44051717,\n",
       "       0.42823727, 0.42524856, 0.42381915, 0.43804826, 0.44545518,\n",
       "       0.46234811, 0.45968429, 0.45351177, 0.45494119, 0.45442139,\n",
       "       0.46007411, 0.46962516, 0.46462221, 0.46234811, 0.46241315,\n",
       "       0.47729193, 0.47631737, 0.47163931, 0.46800083, 0.46702618,\n",
       "       0.46358262, 0.46540185, 0.47189916, 0.47664226, 0.47904623,\n",
       "       0.48411412, 0.48859727, 0.48015076, 0.46715615, 0.46715615,\n",
       "       0.45754017, 0.45760511, 0.46065885, 0.46767595, 0.47326363,\n",
       "       0.46065885, 0.4556559 , 0.45715035, 0.46273803, 0.44909366,\n",
       "       0.44383086, 0.44344103, 0.43421477, 0.43155096, 0.43739849,\n",
       "       0.44415574, 0.44357091, 0.43278546, 0.43142098, 0.41660714,\n",
       "       0.41725691, 0.40991492, 0.41673711, 0.39620561, 0.3894484 ,\n",
       "       0.39113772, 0.38223639, 0.38236632, 0.37612891, 0.3691768 ,\n",
       "       0.37099604, 0.37223054, 0.36696774, 0.35988565, 0.35930092,\n",
       "       0.3438373 , 0.3508544 , 0.37028133, 0.3771685 , 0.3774284 ,\n",
       "       0.37521929, 0.39276204, 0.37723349, 0.37729843, 0.36592815,\n",
       "       0.36735756, 0.38451044, 0.38678449, 0.37918265, 0.37463456,\n",
       "       0.37599899, 0.39230723, 0.38619975, 0.38516016, 0.40367747,\n",
       "       0.40042881, 0.39354169, 0.39406148, 0.40413233, 0.39347675,\n",
       "       0.38613477, 0.38490026, 0.38658958, 0.39438637, 0.38756418,\n",
       "       0.38645965, 0.39113772, 0.40289782, 0.3989994 , 0.39256708,\n",
       "       0.3887337 , 0.3694367 , 0.35670199, 0.37242545, 0.36189985,\n",
       "       0.36027552, 0.35410306, 0.36859207, 0.34299267, 0.32421547,\n",
       "       0.31771817, 0.30342412, 0.36534341, 0.34604641, 0.33253201,\n",
       "       0.32934834, 0.3335716 , 0.33402641, 0.28503673, 0.28217791,\n",
       "       0.30439867, 0.28562147, 0.27665521, 0.21791957, 0.21148726,\n",
       "       0.19303489, 0.15041259, 0.10707559, 0.14846338, 0.1949841 ,\n",
       "       0.14690404, 0.1263076 , 0.15950883, 0.17536222, 0.1827042 ,\n",
       "       0.13273992, 0.12156455, 0.10980444, 0.10785523, 0.11324801,\n",
       "       0.16243259, 0.16763046, 0.17789621, 0.1874472 , 0.19238517,\n",
       "       0.18127479, 0.14917809, 0.15203692, 0.15476577, 0.14001692,\n",
       "       0.11708144, 0.09739457, 0.12617763, 0.11734134, 0.10272241,\n",
       "       0.08979274, 0.05165359, 0.04704047, 0.08615426, 0.10863493,\n",
       "       0.11136378, 0.13891239, 0.09590022, 0.09706973, 0.10421676,\n",
       "       0.10811513, 0.09836918, 0.13631346, 0.14216104, 0.14229096,\n",
       "       0.13170038, 0.11760119, 0.12468328, 0.13696318, 0.14917809,\n",
       "       0.13715809, 0.13637839, 0.11948541, 0.12149961, 0.12279905,\n",
       "       0.1260477 , 0.12020012, 0.12851666, 0.14144633, 0.14781366,\n",
       "       0.16106817, 0.16620104, 0.15008771, 0.14664414, 0.11864078,\n",
       "       0.12546297, 0.12409849, 0.10434673, 0.0949906 , 0.10363203,\n",
       "       0.08414011, 0.08686897, 0.09141706, 0.08751869, 0.10207264,\n",
       "       0.10525635, 0.12325386, 0.11285818, 0.09817426, 0.09232668,\n",
       "       0.09823925, 0.10356704, 0.06906637, 0.11422261, 0.12487819,\n",
       "       0.0997336 , 0.09940877, 0.09063742, 0.10161783, 0.07842245,\n",
       "       0.07263986, 0.07095055, 0.05633161, 0.04866479, 0.0493795 ,\n",
       "       0.05542199, 0.05477227, 0.0435969 , 0.02124621, 0.01650316,\n",
       "       0.019297  , 0.00695211, 0.        , 0.00409333, 0.01474891,\n",
       "       0.03073227, 0.03164189, 0.04957446, 0.05659151, 0.05425248,\n",
       "       0.06477813, 0.07530378, 0.06126958, 0.07283477, 0.08712887,\n",
       "       0.07770775, 0.09239167, 0.09232668, 0.07056072, 0.07764281,\n",
       "       0.07296474, 0.09115716, 0.10116302, 0.09869406, 0.09362617,\n",
       "       0.09382108, 0.11194857, 0.11357289, 0.11032424, 0.1072705 ,\n",
       "       0.11480735, 0.12370867, 0.1055162 , 0.10168281, 0.11025925,\n",
       "       0.10740042, 0.1207849 , 0.1198103 , 0.11474241, 0.12474826,\n",
       "       0.12877656, 0.12747712, 0.13826262, 0.14774867, 0.15275163,\n",
       "       0.1506075 , 0.1581444 , 0.15554547, 0.14781366, 0.13904231,\n",
       "       0.13904231, 0.13676826, 0.13949712, 0.15405107, 0.15145218,\n",
       "       0.13748297, 0.14021183, 0.13787279, 0.14573453, 0.14294068,\n",
       "       0.15320644, 0.17107402, 0.17627188, 0.16672084, 0.17022939,\n",
       "       0.17347804, 0.16854008, 0.17490741, 0.16951468, 0.17841596,\n",
       "       0.17477748, 0.16438181, 0.15905402, 0.15418104, 0.1569099 ,\n",
       "       0.15866415, 0.14391529, 0.14131636, 0.14618934, 0.14599442,\n",
       "       0.15703982, 0.15918394, 0.15703982, 0.1631473 , 0.14722893,\n",
       "       0.1400819 , 0.13579366, 0.12929636, 0.13520893, 0.1315704 ,\n",
       "       0.13312974, 0.14703402, 0.15320644, 0.16711066, 0.17178873,\n",
       "       0.17601199, 0.17750634, 0.18056008, 0.18471835, 0.19225524,\n",
       "       0.19647845, 0.19465922, 0.19413947, 0.20466507, 0.20323565,\n",
       "       0.20927815, 0.21233189, 0.21103245, 0.20999286, 0.21629525,\n",
       "       0.21551555, 0.21025275, 0.21057764, 0.21545062, 0.21181209,\n",
       "       0.2014814 , 0.20304074, 0.20213112, 0.21291667, 0.22428695,\n",
       "       0.2305893 , 0.23286339, 0.22993958, 0.22636609, 0.23110909,\n",
       "       0.22539148, 0.21369632, 0.21096746, 0.21109739, 0.21733484,\n",
       "       0.22928986, 0.23195377, 0.23468263, 0.24156976, 0.23422782,\n",
       "       0.24468848, 0.25099088, 0.25644859, 0.25508416, 0.25053607,\n",
       "       0.2566435 , 0.25268014, 0.24332406, 0.24033525, 0.24514329,\n",
       "       0.25131571, 0.24377887, 0.23292833, 0.22669093, 0.23065428,\n",
       "       0.2443636 , 0.24670263, 0.25177052, 0.25508416, 0.26106167,\n",
       "       0.25768309, 0.26197129, 0.27048279, 0.26723414, 0.27048279,\n",
       "       0.2701579 , 0.26671434, 0.26249108, 0.26333571, 0.25670849,\n",
       "       0.25378472, 0.24202457, 0.245858  , 0.23611205, 0.23377301,\n",
       "       0.23676177, 0.24397378, 0.24910665, 0.25307001, 0.2648951 ,\n",
       "       0.27204213, 0.27626538, 0.27061271, 0.27061271, 0.28081348,\n",
       "       0.28198299, 0.2824378 , 0.27308167, 0.27230203, 0.28263271,\n",
       "       0.27879933, 0.28406213, 0.26762396, 0.26651943, 0.2834774 ,\n",
       "       0.28471185, 0.28061857, 0.27899424, 0.28191801, 0.27399129,\n",
       "       0.27236696, 0.28165811, 0.2826977 , 0.28607628, 0.28523165,\n",
       "       0.2869859 , 0.27925414, 0.27405628, 0.28367231, 0.28802549,\n",
       "       0.2884803 , 0.29172895, 0.29380809, 0.29530249, 0.29283348,\n",
       "       0.28776559, 0.28854528, 0.29725165, 0.3010201 , 0.29939577,\n",
       "       0.3025145 , 0.3062829 , 0.29965567, 0.30063027, 0.30745241,\n",
       "       0.29952569, 0.30205964, 0.29809633, 0.28887012, 0.27282177,\n",
       "       0.27490096, 0.27249694, 0.26788386, 0.265155  , 0.26067185,\n",
       "       0.26249108, 0.27145734, 0.27555068, 0.25547398, 0.24351897,\n",
       "       0.25151062, 0.25449938, 0.25345983, 0.25436945, 0.25605877,\n",
       "       0.26457022, 0.27704503, 0.27899424, 0.28107338, 0.28412712,\n",
       "       0.27561561, 0.27782472, 0.27184722, 0.27944905, 0.28633618,\n",
       "       0.29172895, 0.29185887, 0.29192386, 0.29887597, 0.30517837])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 6s 74ms/step - loss: 0.0302\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0043\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 3s 123ms/step - loss: 0.0032\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 3s 124ms/step - loss: 0.0027\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 3s 109ms/step - loss: 0.0029\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 3s 111ms/step - loss: 0.0028\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 3s 111ms/step - loss: 0.0025\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 3s 128ms/step - loss: 0.0023\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.0026\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 3s 125ms/step - loss: 0.0022\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 0.0021\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 3s 109ms/step - loss: 0.0020\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 3s 109ms/step - loss: 0.0026\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 3s 115ms/step - loss: 0.0020\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 3s 104ms/step - loss: 0.0021\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 3s 104ms/step - loss: 0.0018\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0019\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 2s 100ms/step - loss: 0.0023\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0021\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 2s 103ms/step - loss: 0.0018\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0018\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0019\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 2s 100ms/step - loss: 0.0018\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0019\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0018\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 3s 105ms/step - loss: 0.0018\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 3s 122ms/step - loss: 0.0016\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.0017\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 3s 130ms/step - loss: 0.0015\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 3s 111ms/step - loss: 0.0017\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 0.0017\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 0.0017\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0015\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0015\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0015\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 0.0015\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0015\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 2s 103ms/step - loss: 0.0014\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 2s 103ms/step - loss: 0.0012\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0013\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0013\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0013\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0013\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0013\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 2s 103ms/step - loss: 0.0014\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0013\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 0.0012\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 3s 121ms/step - loss: 0.0013\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 3s 118ms/step - loss: 0.0014\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 0.0012\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 3s 105ms/step - loss: 0.0013\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 3s 105ms/step - loss: 0.0013\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 3s 109ms/step - loss: 0.0013\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 3s 112ms/step - loss: 0.0011\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.0013\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 2s 99ms/step - loss: 0.0012\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0011\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0012\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0011\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 0.0011\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 3s 120ms/step - loss: 0.0011\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 3s 108ms/step - loss: 0.0011\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 3s 116ms/step - loss: 0.0011\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 3s 111ms/step - loss: 0.0012\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 3s 116ms/step - loss: 0.0012\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 3s 113ms/step - loss: 0.0012\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 0.0010\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 9.8485e-04\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 3s 105ms/step - loss: 0.0011\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 3s 111ms/step - loss: 9.7811e-04\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 3s 114ms/step - loss: 0.0011\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 3s 114ms/step - loss: 9.8789e-04\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 3s 119ms/step - loss: 0.0010\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 2s 103ms/step - loss: 0.0010\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 0.0010\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 2s 100ms/step - loss: 9.8235e-04\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 9.9783e-04\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 2s 99ms/step - loss: 0.0011\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 9.1908e-04\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 2s 100ms/step - loss: 9.1182e-04\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 2s 99ms/step - loss: 0.0010\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 8.6174e-04\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 8.6503e-04\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 3s 115ms/step - loss: 0.0010\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 3s 108ms/step - loss: 8.5235e-04\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 9.0485e-04\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 3s 115ms/step - loss: 9.1550e-04\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 3s 105ms/step - loss: 8.5133e-04\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 3s 108ms/step - loss: 8.0824e-04\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 3s 113ms/step - loss: 9.7259e-04\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 3s 110ms/step - loss: 8.5084e-04\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 7.9528e-04\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 2s 104ms/step - loss: 8.9537e-04\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 3s 107ms/step - loss: 8.8415e-04\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 8.2245e-04\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 2s 102ms/step - loss: 8.7691e-04\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 2s 99ms/step - loss: 8.2300e-04\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 99ms/step - loss: 8.3772e-04\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 2s 101ms/step - loss: 8.9024e-04\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 3s 116ms/step - loss: 8.9141e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9800414c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a460708cdad8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'arange' is not defined"
     ]
    }
   ],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "dataset_train = df.iloc[:800, 1:2]\n",
    "dataset_test = df.iloc[800:, 1:2]\n",
    "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in arange(X):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_test.shape)\n",
    "# (459, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_values= np.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
